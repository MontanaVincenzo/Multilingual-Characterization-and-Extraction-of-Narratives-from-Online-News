{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnaOFiOAlfIB",
        "outputId": "6b640d8a-405d-4228-eaaf-62a4bcd579aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Multilingual-Characterization-and-Extraction-of-Narratives-from-Online-News'...\n",
            "remote: Enumerating objects: 2469, done.\u001b[K\n",
            "remote: Counting objects: 100% (2469/2469), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2416/2416), done.\u001b[K\n",
            "remote: Total 2469 (delta 49), reused 2466 (delta 48), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (2469/2469), 9.92 MiB | 17.05 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "/content/Multilingual-Characterization-and-Extraction-of-Narratives-from-Online-News\n"
          ]
        }
      ],
      "source": [
        "# Define repository URL and notebook file name\n",
        "github_username = \"\"\n",
        "repo_name = \"Multilingual-Characterization-and-Extraction-of-Narratives-from-Online-News\"\n",
        "notebook_name = \"ProjectLLMs.ipynb\"\n",
        "# Construct the full repository URL\n",
        "repo_url = f\"https://github.com/{github_username}/{repo_name}.git\"\n",
        "\n",
        "# Clone the repository\n",
        "!git clone {repo_url}\n",
        "\n",
        "# Change to Repository Directory\n",
        "%cd {repo_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7SqFx1TYeRQ"
      },
      "source": [
        "# Data Preprocessing Phase \n",
        "\n",
        "## Note\n",
        "- Preprocessed data are already available in the repository\n",
        "- To execute the code skip directly to the \"Classification Phase\" part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M47U6v4GyR6"
      },
      "source": [
        "# Data Loading Functions Documentation\n",
        "\n",
        "## Overview\n",
        "The `load_data_train_val` and `load_data_test` functions are used to load and preprocess dataset files in tab-separated format (`.tsv`). These functions read data from a file and assign appropriate column names\n",
        "\n",
        "## Parameters\n",
        "- `file_path` (str): The path to the file containing the dataset.\n",
        "\n",
        "## Expected Output\n",
        "- `load_data_train_val(file_path)`: Returns a Pandas DataFrame with labeled training and validation data.\n",
        "- `load_data_test(file_path)`: Returns a Pandas DataFrame with test data.\n",
        "\n",
        "## Note\n",
        "- when referring to validation data here we intend the development samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YLmD7nFsh7K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data_train_val(file_path):\n",
        "  data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"file_name\", \"entity\", \"start_offset\", \"end_offset\", \"label_0\", \"label_1\", \"label_2\", \"label_3\", \"label_4\", \"label_5\", \"label_6\", \"label_7\", \"label_8\", \"label_9\", \"label_10\", \"label_11\", \"label_12\"])\n",
        "  return data\n",
        "\n",
        "def load_data_test(file_path):\n",
        "  data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"file_name\", \"entity\", \"start_offset\", \"end_offset\"])\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_DCa6k9FKZz"
      },
      "outputs": [],
      "source": [
        "# Load the file\n",
        "file_path_train = ...\n",
        "file_path_val = ...\n",
        "train_data = load_data_train_val(file_path_train)\n",
        "val_data = load_data_train_val(file_path_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIN7wLokFOlb"
      },
      "outputs": [],
      "source": [
        "file_path_test = ...\n",
        "test_data = load_data_test(file_path_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcLDHy9NIhjl"
      },
      "source": [
        "# BERT-based Text Vectorization Function\n",
        "\n",
        "## Overview\n",
        "The `get_text_vector` function leverages a pre-trained BERT model to generate a semantic vector representation for a given text input. It tokenizes the text, processes it through BERT, and extracts the representation of the `[CLS]` token as the final vector.\n",
        "\n",
        "## Parameters\n",
        "- `text` (str): The input text to be transformed into a semantic vector.\n",
        "\n",
        "## Expected Output\n",
        "- Returns a PyTorch tensor representing the `[CLS]` token's hidden state from the BERT model.\n",
        "- The output tensor has the shape `(1, hidden_size)`, where `hidden_size` is 768 for `bert-base-cased`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_Sv-pzBFUq7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT (cased) model and tokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "def get_text_vector(text):\n",
        "    # Tokenize the input text and add special tokens [CLS] and [SEP]\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Get the outputs from BERT model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Use the [CLS] token's representation for the semantic vector\n",
        "    cls_vector = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "    return cls_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMluL42VI2jV"
      },
      "source": [
        "# Semantic Vector Extraction Function\n",
        "\n",
        "## Overview\n",
        "The `get_semantic_vector` function reads a text file, retrieves its content, and generates a semantic vector representation using a BERT-based model. It processes the file associated with an entity and converts the text into a vectorial representation.\n",
        "\n",
        "## Parameters\n",
        "- `entity_info` (dict): A dictionary containing entity details, including:\n",
        "  - `\"file_name\"` (str): The name of the file containing the text.\n",
        "  - Additional entity metadata (not used in this function).\n",
        "- `folder_path` (str): The path to the folder where text files are stored.\n",
        "\n",
        "## Expected Output\n",
        "- Returns a PyTorch tensor representing the semantic vector of the text extracted from the file.\n",
        "- The vector is generated using the `get_text_vector` function, which employs a BERT model to obtain the `[CLS]` token's hidden state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWsVN0VUs4mr"
      },
      "outputs": [],
      "source": [
        "def get_semantic_vector(entity_info, folder_path):\n",
        "    \"\"\"\n",
        "    Add special tokens to mark entities in the text based on their offsets.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the .txt file containing the text.\n",
        "        entity_info (list of dict): A list of dictionaries containing the entity offsets and labels.\n",
        "            Example:\n",
        "            [{\"start_offset\": 27, \"end_offset\": 40, \"entity\": \"lab-grown meat\"}]\n",
        "\n",
        "    Returns:\n",
        "        str: The modified text with special tokens.\n",
        "    \"\"\"\n",
        "        # Open and read the file content\n",
        "\n",
        "    with open(folder_path+\"/\"+entity_info[\"file_name\"], \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    semantic_vector = get_text_vector(text)\n",
        "\n",
        "    return semantic_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARicmw8sJCjd"
      },
      "source": [
        "# Entity Annotation with Special Tokens\n",
        "\n",
        "## Overview\n",
        "The `add_special_tokens` function modifies a text file by inserting special tokens (`<T>` and `</T>`) around entity mentions based on their character offsets. This helps highlight entities in the text for further processing.\n",
        "\n",
        "## Parameters\n",
        "- `entity_info` (dict): A dictionary containing entity details, including:\n",
        "  - `\"file_name\"` (str): The name of the text file.\n",
        "  - `\"start_offset\"` (int): The starting character position of the entity in the text.\n",
        "  - `\"end_offset\"` (int): The ending character position of the entity in the text.\n",
        "  - Additional entity metadata (not used in this function).\n",
        "- `folder_path` (str): The path to the folder where the text files are stored.\n",
        "\n",
        "## Expected Output\n",
        "- Returns a modified string where the specified entity is wrapped in `<T>` and `</T>` tags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL8g9yQPFaYl"
      },
      "outputs": [],
      "source": [
        "def add_special_tokens(entity_info, folder_path):\n",
        "    \"\"\"\n",
        "    Add special tokens to mark entities in the text based on their offsets.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the .txt file containing the text.\n",
        "        entity_info (list of dict): A list of dictionaries containing the entity offsets and labels.\n",
        "            Example:\n",
        "            [{\"start_offset\": 27, \"end_offset\": 40, \"entity\": \"lab-grown meat\"}]\n",
        "\n",
        "    Returns:\n",
        "        str: The modified text with special tokens.\n",
        "    \"\"\"\n",
        "        # Open and read the file content\n",
        "\n",
        "    with open(folder_path+\"/\"+entity_info[\"file_name\"], \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Add special tokens to each entity\n",
        "\n",
        "    start, end = entity_info[\"start_offset\"], entity_info[\"end_offset\"]\n",
        "    text = text[:start] + \"<T> \" + text[start:end+1] + \" </T>\" + text[end+1:]\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEhRws8TJJIV"
      },
      "source": [
        "# Training and Test Dataset Preprocessing\n",
        "\n",
        "## Overview\n",
        "The `preprocess_train_val_dataset` and `preprocess_test_dataset` functions process training, validation, and test datasets by extracting relevant entity information, adding special tokens, and generating semantic vectors.\n",
        "\n",
        "## Parameters\n",
        "- `data` (Pandas DataFrame): A dataset containing entity details and labels.\n",
        "- `folder_path` (str): Path to the folder containing text files.\n",
        "\n",
        "## Expected Output\n",
        "- `preprocess_train_val_dataset(data, folder_path)`: Returns a list of dictionaries where each entry includes:\n",
        "  - `file_name`, `entity_name`, `start_offset`, `end_offset`\n",
        "  - `main_role`, `refined_roles`\n",
        "  - `text` with special tokens\n",
        "  - `semantic_vector`\n",
        "  \n",
        "- `preprocess_test_dataset(data, folder_path)`: Returns a list of dictionaries where each entry includes:\n",
        "  - `file_name`, `entity_name`, `start_offset`, `end_offset`\n",
        "  - `text` with special tokens\n",
        "  - `semantic_vector`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiIRdA8lFdq1"
      },
      "outputs": [],
      "source": [
        "def preprocess_train_val_dataset(data, folder_path):\n",
        "  semantic_similarity_dataset = []\n",
        "  for index, row in data.iterrows():\n",
        "    dict_entity = {\"file_name\": row[0], \"entity\": row[1], \"start_offset\": row[2], \"end_offset\": row[3],}\n",
        "    label_list = []\n",
        "    for i in range(5, 17):\n",
        "      if str(row[i]) != 'nan':\n",
        "        label_list.append(row[i])\n",
        "    semantic_similarity_dataset.append({\"file_name\": row[0], \"entity_name\": row[1], \"start_offset\": row[2], \"end_offset\": row[3], \"main_role\": row[4], \"refined_roles\": label_list, \"text\": add_special_tokens(dict_entity, folder_path), \"semantic_vector\": get_semantic_vector(dict_entity, folder_path)})\n",
        "  return semantic_similarity_dataset\n",
        "\n",
        "def preprocess_test_dataset(data, folder_path):\n",
        "  semantic_similarity_dataset = []\n",
        "  for index, row in data.iterrows():\n",
        "    dict_entity = {\"file_name\": row[0], \"entity\": row[1], \"start_offset\": row[2], \"end_offset\": row[3]}\n",
        "    semantic_similarity_dataset.append({\"file_name\": row[0], \"entity_name\": row[1], \"start_offset\": row[2], \"end_offset\": row[3], \"text\": add_special_tokens(dict_entity, folder_path), \"semantic_vector\": get_semantic_vector(dict_entity, folder_path)})\n",
        "  return semantic_similarity_dataset\n",
        "\n",
        "semantic_similarity_train_dataset = preprocess_train_val_dataset(train_data, \"./Datasets/Train/EN/raw-documents\")\n",
        "semantic_similarity_val_dataset = preprocess_train_val_dataset(val_data, \"./Datasets/Development/EN/subtask-1-documents\")\n",
        "semantic_similarity_test_dataset = preprocess_test_dataset(test_data, \"./Datasets/Test/EN/subtask-1-documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQExOPXrJ_e-"
      },
      "source": [
        "# Save Processed Data to JSON\n",
        "\n",
        "## Overview\n",
        "The `save_to_json_train_val` and `save_to_json_test` functions serialize processed datasets into JSON format, ensuring compatibility by converting tensors into lists.\n",
        "\n",
        "## Parameters\n",
        "- `data` (list of dicts): A list of dictionaries containing processed entity details.\n",
        "- `file_path` (str): The destination file path where the JSON output will be saved.\n",
        "\n",
        "## Expected Output\n",
        "- `save_to_json_train_val(data, file_path)`: Saves training and validation data in JSON format, including entity details, roles, text with special tokens, and semantic vectors.\n",
        "- `save_to_json_test(data, file_path)`: Saves test data in JSON format, including entity details, text with special tokens, and semantic vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhTZSKPDFuDp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_to_json_train_val(data, file_path):\n",
        "    # Transform the data into JSON-compatible format if necessary\n",
        "    json_data = []\n",
        "    for entry in data:\n",
        "        # Extract details and prepare JSON object\n",
        "        json_object = {\n",
        "            \"file_name\": entry['file_name'],\n",
        "            \"entity_name\": entry['entity_name'],\n",
        "            \"start_offset\": entry['start_offset'],\n",
        "            \"end_offset\": entry['end_offset'],\n",
        "            \"main_role\": entry['main_role'],\n",
        "            \"refined_roles\": entry['refined_roles'],\n",
        "            \"text\": entry['text'],\n",
        "            \"semantic_vector\": entry['semantic_vector'].tolist()  # Ensure compatibility with JSON\n",
        "        }\n",
        "        json_data.append(json_object)\n",
        "\n",
        "    # Save the list of JSON objects to a file\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(json_data, file, indent=4)  # Pretty-print with indentation\n",
        "\n",
        "    print(f\"Data saved as JSON to {file_path}\")\n",
        "\n",
        "def save_to_json_test(data, file_path):\n",
        "    json_data = []\n",
        "    for entry in data:\n",
        "\n",
        "        # Extract details and prepare JSON object\n",
        "        json_object = {\n",
        "            \"file_name\": entry['file_name'],\n",
        "            \"entity_name\": entry['entity_name'],\n",
        "            \"start_offset\": entry['start_offset'],\n",
        "            \"end_offset\": entry['end_offset'],\n",
        "            \"text\": entry['text'],\n",
        "            \"semantic_vector\": entry['semantic_vector'].tolist()  # Ensure compatibility with JSON\n",
        "        }\n",
        "        json_data.append(json_object)\n",
        "\n",
        "\n",
        "        # Save the list of JSON objects to a file\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(json_data, file, indent=4)  # Pretty-print with indentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4Y7hraeF0MI"
      },
      "outputs": [],
      "source": [
        "preprocess_train_json_path = ...\n",
        "preprocess_val_json_path = ...\n",
        "preprocess_test_json_path = ...\n",
        "\n",
        "save_to_json_train_val(semantic_similarity_train_dataset, preprocess_train_json_path)\n",
        "save_to_json_train_val(semantic_similarity_val_dataset, preprocess_val_json_path)\n",
        "save_to_json_test(semantic_similarity_test_dataset, preprocess_test_json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_lNqn-dYwBT"
      },
      "source": [
        "# Classification Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-8GVqDiKO3w"
      },
      "source": [
        "# Hugging Face Model Authentication and Loading\n",
        "\n",
        "## Overview\n",
        "The script logs into the Hugging Face Model Hub, downloads a pre-trained `Llama-3.2-3B` model, and configures the tokenizer for text generation tasks.\n",
        "\n",
        "## Parameters\n",
        "- `token` (str): Authentication token for accessing the Hugging Face Model Hub.\n",
        "- `model_id` (str): Identifier for the pre-trained model (`meta-llama/Llama-3.2-3B`).\n",
        "\n",
        "## Expected Output\n",
        "- Logs into the Hugging Face Model Hub.\n",
        "- Loads the `Llama-3.2-3B` model into memory using `torch_dtype=torch.float16` and `device_map=\"auto\"` for optimized inference.\n",
        "- Initializes the tokenizer and sets the pad token to match the end-of-sequence (EOS) token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621,
          "referenced_widgets": [
            "183f6d92f98a495ca53e4228ba3f476f",
            "d118b78f077c459986bcab0f72a1131e",
            "8ff8d1b18f2e4c4d9be7ac1efd647653",
            "553dba0372ac4e159352450d2c18851e",
            "9c68e0525339432ab9df157bad026988",
            "9fa48cd94c574738b6517565225829ae",
            "ead4a2c2739240eeb12cd8cd2995d59a",
            "1fa2d0c633f34353b8f1adb036a3dbdf",
            "2285895a068942beb02d603b0153b0d7",
            "573c3918a3bd408881f2b71a7951bebc",
            "4bf0fbc33dc8434f84ac5646d24d4340",
            "bde07c073b7a4dc29a4caad15c4adbd2",
            "f75e25a82cd841a4adb09af09f2fab30",
            "2faa2c541b534c648eb0d83ef0539eed",
            "2c504dbe585b47c0bb9071c9b39bfddd",
            "4f109858192d4658881f2b6afcad02ba",
            "242daab895514656b2922a4dbb6a3a16",
            "5bcad412518440068081e57089253a45",
            "1eef2cd81dec43db914a6a6df25b22eb",
            "a9214c1f98c7488eb676422ed71347e0",
            "f03cddf9f42f4f7eb661b7c1d8f6514a",
            "051f1321d96a4f1187fdbb1ab21fc8fb",
            "e1262702b868466bb65fa81e20b16553",
            "be7a10388759430fbf1017b6bfe91ea5",
            "4ff0d07a2c3342c29b6c39ac0947e2ee",
            "decba6148a114addb7bb2d5237fe6c68",
            "187f5c18f080495a94023c8e13952496",
            "e7f6f476e71b4a22a50d351e059e5206",
            "28eadc048d454e8cab5a92f882850d97",
            "f37e0d5a6cc243629b56eca82589597b",
            "eaea09a0359b4fc9b98478ae39bf2c80",
            "9772a998fecf4fcdb38350c33f2d2ca1",
            "d631df91a5234adaa8d9b919a8c3ab15",
            "3ec779308b27465dbff3d7184e4a93a6",
            "9619890a23b74241a430e6f43007e504",
            "f93831b3b6be492f9349c5a63c8c52b8",
            "2b8a060125d748aa9ed490eb0ffe90f0",
            "eac20711608f4103a1b2ece366f7955e",
            "7f5326ab73aa4e8686519361408ac48d",
            "9e94a829379a42f6877d032b65d921df",
            "42f44926f5404158accb21489fe9d863",
            "cc1d0eee2a8d40c09973c68faa3f569b",
            "6590290ef63e47f5a6571b16cc9f367c",
            "670b7a5122844f64b41d9752c514667b"
          ]
        },
        "id": "wHowe3uZJo04",
        "outputId": "f5867f36-18e4-4125-ad76-818c9db821e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "183f6d92f98a495ca53e4228ba3f476f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bde07c073b7a4dc29a4caad15c4adbd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1262702b868466bb65fa81e20b16553",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ec779308b27465dbff3d7184e4a93a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-91003529af52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3943\u001b[0m             \u001b[0;31m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3944\u001b[0;31m             resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n\u001b[0m\u001b[1;32m   3945\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3946\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# Load from URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             cached_filename = cached_file(\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mshard_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         http_get(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Login to the Hugging Face model hub to be able to upload models\n",
        "token = \"\"\n",
        "login(token=token)\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I6kvyrQKelv"
      },
      "source": [
        "# Read JSON Data into List of Dictionaries\n",
        "\n",
        "## Overview\n",
        "The `read_file_into_list_of_dicts` function loads JSON-formatted data, reconstructs its structure, and converts semantic vectors back into PyTorch tensors for further processing.\n",
        "\n",
        "## Parameters\n",
        "- `path_to_file` (str): The path to the JSON file containing the dataset.\n",
        "- `train_val` (bool, default=True): A flag indicating whether the dataset is for training/validation (`True`) or testing (`False`).\n",
        "\n",
        "## Expected Output\n",
        "- Returns a list of dictionaries where each dictionary represents an entity with relevant attributes:\n",
        "  - **Training/Validation Format:** Includes `file_name`, `entity_name`, `main_role`, `refined_roles`, `text`, and `semantic_vector`.\n",
        "  - **Test Format:** Includes `file_name`, `start_offset`, `end_offset`, `text`, and `semantic_vector`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JLAxtTNYyydK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def read_file_into_list_of_dicts(path_to_file, train_val=True):\n",
        "    rows = []\n",
        "\n",
        "    # Open and read the JSON file\n",
        "    with open(path_to_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)  # Load the JSON file as a list of dictionaries\n",
        "\n",
        "    for entry in data:\n",
        "        file_name = entry['file_name']\n",
        "        semantic_vector = torch.tensor(entry['semantic_vector'])  # Convert list to tensor\n",
        "        text = entry['text']\n",
        "\n",
        "        if train_val:  # Train / Val format\n",
        "            main_role = entry['main_role']\n",
        "            refined_roles = entry['refined_roles']\n",
        "            entity_name = entry['entity_name']\n",
        "\n",
        "            row_dict = {\n",
        "                'file_name': file_name,\n",
        "                'entity_name': entity_name,\n",
        "                'main_role': main_role.lower(),\n",
        "                'refined_roles': refined_roles,\n",
        "                'text': text,\n",
        "                'semantic_vector': semantic_vector\n",
        "\n",
        "            }\n",
        "\n",
        "        else:  # Test format\n",
        "            start_offset = entry['start_offset']\n",
        "            end_offset = entry['end_offset']\n",
        "\n",
        "            row_dict = {\n",
        "                'file_name': file_name,\n",
        "                'entity_name': entry['entity_name'],\n",
        "                'start_offset': start_offset,\n",
        "                'end_offset': end_offset,\n",
        "                'main_role': '',\n",
        "                'refined_roles': '',\n",
        "                'text': text,\n",
        "                'semantic_vector': semantic_vector\n",
        "            }\n",
        "\n",
        "        rows.append(row_dict)\n",
        "\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYycjIT6K0AA"
      },
      "source": [
        "# Compute Cosine Similarities\n",
        "\n",
        "## Overview\n",
        "The `compute_cosine_similarities` function calculates the cosine similarity between a given query vector and a set of precomputed semantic vectors from training data. It assigns similarity scores to each entry in the dataset.\n",
        "\n",
        "## Parameters\n",
        "- `query_vector` (torch.Tensor): A tensor representing the semantic vector of the query.\n",
        "- `train_data_semantic_vectors` (list of dicts): A list where each dictionary contains a `semantic_vector` representing an entity.\n",
        "\n",
        "## Expected Output\n",
        "- Returns an updated list of dictionaries where each entry includes a new key, `cosine_similarity`, representing the similarity score between the query vector and the respective training vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "17z-UoIJ-4eC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "def compute_cosine_similiraties(query_vector, train_data_semantic_vectors):\n",
        "  train_data_vectors = [item['semantic_vector'].flatten() for item in train_data_semantic_vectors]\n",
        "  # Stack the semantic vectors into a single tensor\n",
        "  semantic_vectors_tensor = torch.stack(train_data_vectors)\n",
        "  # Ensure query_vector is 2D for cosine_similarity\n",
        "  query_vector_2d = query_vector.cpu().detach().numpy().reshape(1, -1)\n",
        "\n",
        "  # Convert semantic_vectors_tensor to numpy and ensure it's 2D\n",
        "  semantic_vectors_2d = semantic_vectors_tensor.cpu().detach().numpy()\n",
        "\n",
        "  # Compute cosine similarities\n",
        "  similarities = cosine_similarity(query_vector_2d, semantic_vectors_2d)\n",
        "\n",
        "  # Assign similarities back to the original list\n",
        "  for i, similarity in enumerate(similarities[0]):\n",
        "      train_data_semantic_vectors[i]['cosine_similarity'] = similarity.item()\n",
        "\n",
        "  return train_data_semantic_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoGOIZkJK_4N"
      },
      "source": [
        "# Role Encoding and Decoding\n",
        "\n",
        "## Overview\n",
        "The `roles_to_binary` and `binary_to_roles` functions facilitate the conversion between refined roles and binary vectors. These functions help encode refined roles into a binary format and decode them back into their respective roles, ensuring consistency with predefined main role mappings.\n",
        "\n",
        "## Functions & Parameters\n",
        "\n",
        "### `roles_to_binary(refined_roles, main_roles_mapping)`\n",
        "- `refined_roles` (list): A list of refined roles associated with an entity.\n",
        "- `main_roles_mapping` (dict): A dictionary where each key is a main role, and each value is a list of refined roles under that category.\n",
        "\n",
        "### `binary_to_roles(binary_dict, main_roles_mapping)`\n",
        "- `binary_dict` (dict): A dictionary containing a single main role and its associated binary vector.\n",
        "- `main_roles_mapping` (dict): The mapping of main roles to their refined roles.\n",
        "\n",
        "## Expected Output\n",
        "- `roles_to_binary(refined_roles, main_roles_mapping)`: Returns a binary vector indicating whether each refined role under the identified main role is present (`Yes`) or absent (`No`).\n",
        "- `binary_to_roles(binary_dict, main_roles_mapping)`: Returns a list of refined roles that correspond to the binary vector representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dAKJkfeFQ7zP"
      },
      "outputs": [],
      "source": [
        "def roles_to_binary(refined_roles, main_roles_mapping):\n",
        "    \"\"\"\n",
        "    Converts refined roles into a binary vector for a single main role.\n",
        "\n",
        "    Args:\n",
        "        refined_roles (list): Refined roles for the current test sample (e.g., [\"Guardian\", \"Rebel\"]).\n",
        "        main_roles_mapping (dict): A dictionary where keys are main roles and values are their refined roles.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with a single main role and its binary vector, or an empty dictionary if inconsistent.\n",
        "    \"\"\"\n",
        "    # Identifica il main role a cui appartengono i refined roles\n",
        "    valid_main_role = None\n",
        "    for main_role, refined_list in main_roles_mapping.items():\n",
        "        if all(role in refined_list for role in refined_roles):\n",
        "            valid_main_role = main_role\n",
        "            break\n",
        "\n",
        "    # Se non sono coerenti con un main role, ritorna un dizionario vuoto\n",
        "    if valid_main_role is None:\n",
        "        raise ValueError(\"Refined roles belong to multiple or invalid main roles.\")\n",
        "\n",
        "    # Costruisce il vettore binario per il main role identificato\n",
        "    binary_vector = ['Yes' if role in refined_roles else 'No' for role in main_roles_mapping[valid_main_role]]\n",
        "    return binary_vector\n",
        "\n",
        "\n",
        "def binary_to_roles(binary_dict, main_roles_mapping):\n",
        "    \"\"\"\n",
        "    Converts a binary vector back into refined roles for a single main role.\n",
        "\n",
        "    Args:\n",
        "        binary_dict (dict): A dictionary with a single main role and its binary vector.\n",
        "        main_roles_mapping (dict): A dictionary where keys are main roles and values are their refined roles.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of refined roles for the given main role.\n",
        "    \"\"\"\n",
        "    # Estrai il main role e il vettore binario\n",
        "    if len(binary_dict) != 1:\n",
        "        raise ValueError(\"Binary dictionary must have exactly one main role.\")\n",
        "\n",
        "    main_role = list(binary_dict.keys())[0]\n",
        "    binary_vector = binary_dict[main_role]\n",
        "\n",
        "    # Converti il vettore binario nei refined roles\n",
        "    refined_roles = [\n",
        "        role for role, flag in zip(main_roles_mapping[main_role], binary_vector) if flag == 1\n",
        "    ]\n",
        "    return refined_roles\n",
        "\n",
        "\n",
        "# Esempio di utilizzo\n",
        "main_roles_mapping = {\n",
        "    \"protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n",
        "    \"antagonist\": [\n",
        "        \"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\",\n",
        "        \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"\n",
        "    ],\n",
        "    \"innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XtP28L-LqUa"
      },
      "source": [
        "# Retrieve Top K Entries per Main Role\n",
        "\n",
        "## Overview\n",
        "The `get_top_k_per_main_role` function selects the top `k` entries from a dataset for each main role (`Protagonist`, `Antagonist`, and `Innocent`) based on cosine similarity. It ensures that each file name appears only once across roles.\n",
        "\n",
        "## Parameters\n",
        "- `train_data_with_vectors` (list of dicts): A list of dictionaries where each entry contains:\n",
        "  - `file_name` (str): Identifier for the document.\n",
        "  - `main_role` (str): The main role category (`Protagonist`, `Antagonist`, or `Innocent`).\n",
        "  - `cosine_similarity` (float): The similarity score used for ranking.\n",
        "  - `text` (str): The textual content associated with the document.\n",
        "- `k` (int): The number of top entries to retrieve per main role.\n",
        "\n",
        "## Expected Output\n",
        "- Returns a list of dictionaries containing the top `k` entries for each main role.\n",
        "- Each dictionary in the output includes:\n",
        "  - `file_name`\n",
        "  - `main_role`\n",
        "  - `cosine_similarity`\n",
        "  - `text`\n",
        "- The returned list contains `k` elements per role (up to 3 roles), totaling at most `3 * k` entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HmBVs7b4S3M3"
      },
      "outputs": [],
      "source": [
        "def get_top_k_per_main_role(train_data_with_vectors, k):\n",
        "  file_names_list = []\n",
        "\n",
        "  #Ricordare di fare lista [protagonist_data, antagonist_data, innocent_data] e poi shufflare\n",
        "  protagonist_data = [item for item in train_data_with_vectors if item.get('main_role') == 'Protagonist']\n",
        "\n",
        "  unique_protagonists = {item['file_name']: item for item in protagonist_data}.values()\n",
        "  # Sort the data by 'cosine_similarity' in descending order and select the top 2\n",
        "  top_2_protagonists = sorted(unique_protagonists, key=lambda x: x['cosine_similarity'], reverse=True)[:k]\n",
        "  top_2_protagonists_file_names = [item['file_name'] for item in top_2_protagonists]\n",
        "\n",
        "  file_names_list.extend(top_2_protagonists_file_names)\n",
        "\n",
        "  antagonist_data = [\n",
        "    item for item in train_data_with_vectors\n",
        "    if item.get('main_role') == 'Antagonist' and item['file_name'] not in file_names_list\n",
        "  ]\n",
        "\n",
        "  unique_antagonists = {item['file_name']: item for item in antagonist_data}.values()\n",
        "\n",
        "  # Sort the remaining data by 'cosine_similarity' in descending order and select the top 2\n",
        "  top_2_antagonists = sorted(unique_antagonists, key=lambda x: x['cosine_similarity'], reverse=True)[:k]\n",
        "\n",
        "  # Retrieve just the 'file_name' field\n",
        "  top_2_antagonists_file_names = [item['file_name'] for item in top_2_antagonists]\n",
        "  top_2_antagonists_texts = [item['text'] for item in top_2_antagonists]\n",
        "\n",
        "  # Append the new file names to the existing list\n",
        "  file_names_list.extend(top_2_antagonists_file_names)\n",
        "\n",
        "\n",
        "  innocent_data = [\n",
        "      item for item in train_data_with_vectors\n",
        "      if item.get('main_role') == 'Innocent' and item['file_name'] not in file_names_list\n",
        "  ]\n",
        "\n",
        "  unique_innocents = {item['file_name']: item for item in innocent_data}.values()\n",
        "\n",
        "\n",
        "\n",
        "  top_2_innocents = sorted(unique_innocents, key=lambda x: x['cosine_similarity'], reverse=True)[:k]\n",
        "\n",
        "  top2_list = []\n",
        "  top2_list.extend(top_2_protagonists)\n",
        "  top2_list.extend(top_2_antagonists)\n",
        "  top2_list.extend(top_2_innocents)\n",
        "  return top2_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN2Wii0HLtLy"
      },
      "source": [
        "# Retrieve Top Examples for Each Refined Role\n",
        "\n",
        "## Overview\n",
        "The `get_refined_roles_examples` function extracts the top example for each refined role within a specified main role category (`Protagonist`, `Antagonist`, or `Innocent`). It ranks the entries based on cosine similarity and ensures that each file appears only once in the results.\n",
        "\n",
        "## Parameters\n",
        "- `train_data_with_vectors` (list of dicts): A dataset where each entry contains:\n",
        "  - `file_name` (str): Identifier for the document.\n",
        "  - `main_role` (str): The main role category.\n",
        "  - `refined_roles` (list): A list of refined roles associated with the entry.\n",
        "  - `cosine_similarity` (float): The similarity score used for ranking.\n",
        "- `main_role` (str): The main role for which refined role examples should be retrieved. Must be one of:\n",
        "  - `\"protagonist\"`\n",
        "  - `\"antagonist\"`\n",
        "  - `\"innocent\"`\n",
        "\n",
        "## Expected Output\n",
        "- Returns a list of dictionaries containing the top example for each refined role within the given main role.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RFlESG6OTb_D"
      },
      "outputs": [],
      "source": [
        "def get_refined_roles_examples(train_data_with_vectors, main_role):\n",
        "  main_roles_mapping = {\n",
        "    \"protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n",
        "    \"antagonist\": [\n",
        "        \"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\",\n",
        "        \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"\n",
        "    ],\n",
        "    \"innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n",
        "  }\n",
        "  data = [item for item in train_data_with_vectors if item.get('main_role') == main_role]\n",
        "  data = sorted(data, key=lambda x: x['cosine_similarity'], reverse=True)\n",
        "  list_files_retrieved = []\n",
        "  list_of_refined_roles = main_roles_mapping[main_role]\n",
        "\n",
        "\n",
        "  # Dictionary to store the top element for each refined role\n",
        "  refined_roles = []\n",
        "\n",
        "  # Iterate through each refined role\n",
        "  for refined_role in list_of_refined_roles:\n",
        "      # Find the top element for the current refined role\n",
        "      top_1_for_current_role = None\n",
        "      for item in data:\n",
        "          # Check if the item's refined_role matches the current refined_role\n",
        "          # and if its file_name is not already in list_files_retrieved\n",
        "          if refined_role in item.get('refined_roles') and item['file_name'] not in list_files_retrieved:\n",
        "              top_1_for_current_role = item\n",
        "              break\n",
        "\n",
        "      # If a valid item was found, add it to the results\n",
        "      if top_1_for_current_role:\n",
        "          refined_roles.append(top_1_for_current_role)\n",
        "          # Add the file_name to the retrieved list\n",
        "          list_files_retrieved.append(top_1_for_current_role['file_name'])\n",
        "\n",
        "  return refined_roles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdAYccOZN1h7"
      },
      "source": [
        "# Role Classification Prompt Generator\n",
        "\n",
        "## Overview\n",
        "The `create_prompt` function generates structured prompts for role classification tasks based on given narratives. It supports two levels of classification:\n",
        "1. **Main Role Classification**  Assigns an entity to one of three broad categories: **innocent, protagonist, or antagonist**.\n",
        "2. **Refined Roles Classification**  Assigns an entity to more specific roles within the main category.\n",
        "\n",
        "The function formats the input examples and test sample into a structured textual prompt.\n",
        "\n",
        "## Parameters\n",
        "- **`context`** *(str)*: A textual description providing background for the task.\n",
        "- **`examples`** *(list of dicts)*: A list of example cases, each containing:\n",
        "  - `text` *(str)*: The narrative where an entity appears.\n",
        "  - `entity_name` *(str)*: The name of the entity being classified.\n",
        "  - `main_role` *(str, optional)*: The main classification of the entity (only required for refined classification).\n",
        "  - `refined_roles` *(list of str, optional)*: A list of refined roles associated with the entity (only required for refined classification).\n",
        "- **`test_sample`** *(dict)*: A dictionary representing the test case to classify, containing:\n",
        "  - `text` *(str)*: The narrative for classification.\n",
        "  - `entity_name` *(str)*: The entity to classify.\n",
        "- **`type_role`** *(str)*: Defines the classification type:\n",
        "  - `\"main\"`: Classifies the entity as **innocent, protagonist, or antagonist**.\n",
        "  - `\"refined\"`: Classifies the entity into **specific roles** within the main category.\n",
        "\n",
        "## Expected Output\n",
        "The function returns a **formatted prompt** for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v0Xdn62_hk_l"
      },
      "outputs": [],
      "source": [
        "main_roles_mapping = {\n",
        "    \"protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n",
        "    \"antagonist\": [\n",
        "        \"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\",\n",
        "        \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"\n",
        "    ],\n",
        "    \"innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n",
        "}\n",
        "\n",
        "def create_prompt(context, examples, test_sample, type_role):\n",
        "  prompt = context\n",
        "  prompt +=\"\\n\\n\\nExample Section:\\n\"\n",
        "\n",
        "  if type_role == 'main':\n",
        "    for i in range(len(examples)):\n",
        "      prompt += f\"\"\"### Example {i+1}\\n\"\"\"\n",
        "      prompt+=f\"\"\"**Narrative**: {examples[i]['text']}\\n\\n\"\"\"\n",
        "      prompt+=f\"\"\"**Main role**: entity {examples[i]['entity_name']} is {examples[i]['main_role']}\\n\\n\\n\"\"\"\n",
        "\n",
        "    prompt +=f\"\"\"End of Example Section\\n\\n\\n\"\"\"\n",
        "\n",
        "    prompt +=f\"\"\"### Your Task\\nNow choose just one **Main Role** between innocent, protagonist, antagonist for the entity framed between <T> and </T>.\"\"\"\n",
        "\n",
        "    prompt +=f\"\"\"**Narrative**: {test_sample['text']}\\n\\n\"\"\"\n",
        "    prompt+= f\"\"\"**Main role**: entity {test_sample['entity_name']} is \"\"\"\n",
        "\n",
        "\n",
        "  elif type_role == 'refined':\n",
        "\n",
        "    refined_roles = main_roles_mapping[examples[0]['main_role']]\n",
        "\n",
        "    for i in range(len(examples)):\n",
        "      binary_vector = roles_to_binary(examples[i]['refined_roles'], main_roles_mapping)\n",
        "\n",
        "      binary_string = ''\n",
        "\n",
        "      for j in range(len(refined_roles)):\n",
        "        binary_string+=f\"\"\"{refined_roles[j]}: {binary_vector[j]}\\n\"\"\"\n",
        "\n",
        "      prompt += f\"\"\"### Example {i+1}:\\n**Narrative**:\\n{examples[i]['text']}\"\"\"\n",
        "\n",
        "      prompt += f\"\"\"\\n\\n**Entity**: {examples[i]['entity_name']}\"\"\"\n",
        "\n",
        "      prompt += f\"\"\"\\n**Refined roles**:\\n{binary_string}\"\"\"\n",
        "\n",
        "      prompt += \"\\n\\n\\n\"\n",
        "\n",
        "    prompt +=\"\\nEnd of Example Section\\n\\n\"\n",
        "\n",
        "    prompt +=\"### Your Task\\n\"\n",
        "\n",
        "    prompt+= '''Now for each of the **Refined Roles**, answer \"Yes\" if the entity framed plays the Refined Role or answer \"No\" if the entity does not play the Refined Role in the following Narrative.\\n\\n'''\n",
        "\n",
        "    prompt += f\"\"\"**Narrative**:\\n{test_sample['text']}\"\"\"\n",
        "\n",
        "    prompt += f\"\"\"\\n\\n**Entity**: {test_sample['entity_name']}\"\"\"\n",
        "\n",
        "    prompt += \"\\n\\n**Refined roles**:\"\n",
        "\n",
        "\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeQ7TdwHO0Fi"
      },
      "source": [
        "# Retrieve Refined Roles Descriptions\n",
        "\n",
        "## Overview\n",
        "The `get_refined_roles_descriptions` function retrieves textual descriptions of refined roles associated with a specified **main role**. It reads the content from a provided file path and returns the file's content as a string. This function is useful for dynamically loading role definitions from an external source.\n",
        "\n",
        "## Parameters\n",
        "- **`main_role`** *(str)*: The main role category for which refined role descriptions are needed. This parameter is currently unused within the function.\n",
        "- **`file_path`** *(str)*: The path to the file containing refined role descriptions.\n",
        "\n",
        "## Expected Output\n",
        "- If the file exists, the function returns its **entire content** as a string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GGLiaQLJssGc"
      },
      "outputs": [],
      "source": [
        "def get_refined_roles_descriptions(main_role, file_path):\n",
        "  try:\n",
        "      with open(file_path, 'r', encoding='utf-8') as file:\n",
        "          content = file.read()  # Legge il contenuto del file come testo grezzo\n",
        "      return content\n",
        "\n",
        "  except FileNotFoundError:\n",
        "      return \"Errore: Il file non  stato trovato.\"\n",
        "\n",
        "  except Exception as e:\n",
        "      return f\"Errore: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qAlD_SIPc8a"
      },
      "source": [
        "# Visualization of the main roles tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AU3vuaRnpgH"
      },
      "outputs": [],
      "source": [
        "print(\"protagonist tokens:\\n\")\n",
        "for el in tokenizer.encode(\"protagonist\"):\n",
        "  print('\\t', tokenizer.decode(el) , ' --> ', el)\n",
        "\n",
        "print(\"antagonist tokens:\\n\")\n",
        "for el in tokenizer.encode(\"antagonist\"):\n",
        "  print('\\t', tokenizer.decode(el) , ' --> ', el)\n",
        "\n",
        "print(\"innocent tokens:\\n\")\n",
        "for el in tokenizer.encode(\"innocent\"):\n",
        "  print('\\t', tokenizer.decode(el) , ' --> ', el)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYvCy_dH9oTA"
      },
      "source": [
        "# Main Role Prediction\n",
        "\n",
        "## Overview\n",
        "The `get_main_role_prediction` function predicts the **main role** of an entity in a given text prompt. It classifies the entity as one of three roles:\n",
        "- **Protagonist**\n",
        "- **Antagonist**\n",
        "- **Innocent**\n",
        "\n",
        "This function uses a **memory-efficient** approach with PyTorch and CUDA to process the input prompt, generate predictions iteratively, and determine the most likely role.\n",
        "\n",
        "## Parameters\n",
        "- **`prompt`** *(str)*: The input text containing context, persona, examples and the entity whose main role needs to be classified.\n",
        "\n",
        "## Expected Output\n",
        "- Returns one of the following role classifications as a **string**:\n",
        "  - `\"Protagonist\"`\n",
        "  - `\"Antagonist\"`\n",
        "  - `\"Innocent\"`\n",
        "\n",
        "## Note\n",
        "- We look at the first **uncased** token of each main role. This choice was taken to increase the probability of the Innocent label (In LLaMa \"Innocent\" has \"In\" as first token, while \"innocent\" starts with \"inn\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RjQSBYbp6izz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_main_role_prediction(prompt):\n",
        "    \"\"\"\n",
        "    Predicts the main role (Protagonist, Antagonist, or Innocent) in a memory-efficient way.\n",
        "    \"\"\"\n",
        "    first_id_protagonist = tokenizer.encode(\"protagonist\")[1]\n",
        "    first_id_antagonist = tokenizer.encode(\"antagonist\")[1]\n",
        "    first_id_innocent = tokenizer.encode(\"innocent\")[1]\n",
        "    # Tokenize prompt and move to CUDA\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    eos_token_id = tokenizer.eos_token_id  # End-of-sequence token\n",
        "\n",
        "    # Parameters for the generation loop\n",
        "    max_length = 500\n",
        "    i = 0\n",
        "\n",
        "    mainrole = \"\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():  # More efficient than torch.no_grad()\n",
        "        while i < max_length and (eos_token_id is None or input_ids[0, -1] != eos_token_id):\n",
        "            output = model(input_ids=input_ids)\n",
        "\n",
        "            # Select logits for specific role tokens (already on CUDA)\n",
        "            next_tokek_Pro = output.logits[:, -1, first_id_protagonist]#4490\n",
        "            next_tokek_Ant = output.logits[:, -1, first_id_antagonist]#519\n",
        "            next_tokek_In = output.logits[:, -1, first_id_innocent]#6258\n",
        "\n",
        "            # Define Temperature\n",
        "            Temperature = 1.0\n",
        "            next_tokek_Pro /= Temperature\n",
        "            next_tokek_Ant /= Temperature\n",
        "            next_tokek_In /= Temperature\n",
        "\n",
        "            # Stack logits & apply softmax\n",
        "            selected_logits = torch.stack([next_tokek_Pro, next_tokek_Ant, next_tokek_In], dim=-1)\n",
        "            probabilities = F.softmax(selected_logits, dim=-1)\n",
        "            # print(probabilities)\n",
        "            argmax_indices = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "            # Assign Main Role Based on Predicted Token\n",
        "            if argmax_indices.item() == 0:\n",
        "                mainrole = \"Protagonist\"\n",
        "                break\n",
        "            elif argmax_indices.item() == 1:\n",
        "                mainrole = \"Antagonist\"\n",
        "                break\n",
        "            elif argmax_indices.item() == 2:\n",
        "                mainrole = \"Innocent\"\n",
        "                break\n",
        "\n",
        "            # Free up memory efficiently\n",
        "            del output\n",
        "            input_ids = input_ids.to(\"cpu\")\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Prevent infinite loop\n",
        "            i += 1\n",
        "\n",
        "    return mainrole\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjDtVKlaP5Jz"
      },
      "source": [
        "# Refined Role Prediction\n",
        "\n",
        "## Overview\n",
        "The `get_refined_roles_predictions` function predicts **refined roles** for an entity in a given text prompt. It determines whether an entity plays each refined role by generating a **Yes/No** classification. This function optimizes memory usage with PyTorch and CUDA for efficient processing.\n",
        "\n",
        "## Parameters\n",
        "- **`prompt`** *(str)*: The input text containing the entity whose refined roles need to be classified.\n",
        "- **`refined_roles`** *(list of str)*: A list of refined role names that the model will evaluate.\n",
        "\n",
        "## Expected Output\n",
        "- Returns a **list of refined roles** *(list of str)* where the entity is classified as playing the role.\n",
        "- If no refined role is assigned based on the probability threshold, the function assigns the role with the highest probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3bspAiOF2Kjz"
      },
      "outputs": [],
      "source": [
        "def get_refined_roles_predictions(prompt, refined_roles):\n",
        "    \"\"\"\n",
        "    Generates refined role predictions (Yes/No) in a memory-efficient way.\n",
        "    \"\"\"\n",
        "    # Tokenize prompt and move to CUDA\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    eos_token_id = tokenizer.eos_token_id  # End-of-sequence token\n",
        "\n",
        "    # Precompute token IDs and move to CUDA (avoid repeated calls)\n",
        "    Yes_id = torch.tensor(tokenizer.encode('Yes')[1:]).to('cuda').unsqueeze(0)\n",
        "    No_id = torch.tensor(tokenizer.encode('No')[1:]).to('cuda').unsqueeze(0)\n",
        "    space_id = torch.tensor(tokenizer.encode(' ')[1:]).to('cuda').unsqueeze(0)\n",
        "    points_id = torch.tensor(tokenizer.encode(':')[1:]).to('cuda').unsqueeze(0)\n",
        "    new_line_id = torch.tensor(tokenizer.encode('\\n')[1:]).to('cuda').unsqueeze(0)\n",
        "\n",
        "    predicted_refined_roles = []\n",
        "\n",
        "    torch.cuda.empty_cache()  # Free unused GPU memory before starting\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    max_prob=0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():  # More efficient than torch.no_grad()\n",
        "        for refined_role in refined_roles:\n",
        "\n",
        "            # Append refined role formatting\n",
        "            concat_input_ids = torch.cat((input_ids, new_line_id), dim=-1)\n",
        "\n",
        "            refined_role_ids = torch.tensor(tokenizer.encode(refined_role)[1:]).to('cuda').unsqueeze(0)\n",
        "            contact_input_ids = torch.cat((concat_input_ids, refined_role_ids, points_id, space_id), dim=-1)\n",
        "\n",
        "            # Run model inference\n",
        "            output = model(input_ids=concat_input_ids)\n",
        "\n",
        "            # Get logits for Yes/No\n",
        "            next_token_Yes = output.logits[:, -1, 9642]\n",
        "            next_token_No = output.logits[:, -1, 2822]\n",
        "\n",
        "            # Stack and apply softmax\n",
        "            selected_logits = torch.stack([next_token_Yes, next_token_No], dim=-1)\n",
        "            probabilities = F.softmax(selected_logits, dim=-1)\n",
        "            argmax_indices = torch.argmax(probabilities, dim=-1)\n",
        "            # print(probabilities)\n",
        "            if(probabilities[0][0]>max_prob):\n",
        "              max_prob=probabilities[0][0]\n",
        "              max_prob_role=refined_role\n",
        "\n",
        "            # Append prediction based on highest probability\n",
        "            if argmax_indices.item() == 0:\n",
        "                predicted_refined_roles.append(refined_role)\n",
        "\n",
        "            del input_ids\n",
        "            input_ids = concat_input_ids\n",
        "            # Free up memory\n",
        "            del output, concat_input_ids, refined_role_ids\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    if len(predicted_refined_roles)==0:\n",
        "      predicted_refined_roles.append(max_prob_role)\n",
        "    return predicted_refined_roles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRo7mUKnQ_s0"
      },
      "source": [
        "# Main and Refined Role Classification System\n",
        "\n",
        "## Overview\n",
        "This script performs a **two-stage classification** of entities in a narrative. It first predicts the **main role** of an entity as either:\n",
        "- **Protagonist**\n",
        "- **Antagonist**\n",
        "- **Innocent**\n",
        "\n",
        "After determining the main role, the script further assigns **refined roles** specific to the chosen main category. The classification is based on **semantic vector similarity** and **LLM-based inference**.\n",
        "\n",
        "The script evaluates classification performance by comparing predictions with actual labels from a validation dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## Process Workflow\n",
        "\n",
        "### **1. Load Training and Validation Data**\n",
        "- The script reads the training (`semantic_vectors_train.json`) and validation (`semantic_vectors_development.json`) datasets.\n",
        "- These datasets contain **semantic vectors** representing entities and their roles.\n",
        "\n",
        "### **2. Compute Cosine Similarity for Example Selection**\n",
        "- For each validation entity, the script computes the **cosine similarity** between its semantic vector and all training vectors.\n",
        "- It retrieves the **top `k=2` most similar examples** per main role.\n",
        "\n",
        "### **3. Generate Main Role Classification Prompt**\n",
        "- A structured prompt is created to classify the entity as **Protagonist, Antagonist, or Innocent**.\n",
        "- The prompt includes:\n",
        "  - A **persona** assignment\n",
        "  - A **task description** explaining how to classify entities.\n",
        "  - **Example cases** retrieved based on cosine similarity.\n",
        "\n",
        "### **4. Predict Main Role Using LLM**\n",
        "- The script uses `get_main_role_prediction(prompt)` to determine the entitys **main role**.\n",
        "- If the prediction matches the actual label, the **correctly_predicted** counter is incremented.\n",
        "\n",
        "### **5. Retrieve and Describe Refined Roles**\n",
        "- Based on the predicted **main role**, the script retrieves the corresponding **refined roles**:\n",
        "  - **Protagonist**  Guardian, Martyr, Peacemaker, Rebel, Underdog, Virtuous\n",
        "  - **Antagonist**  Instigator, Conspirator, Tyrant, Foreign Adversary, Traitor\n",
        "  - **Innocent**  Forgotten, Exploited, Victim, Scapegoat\n",
        "- It loads descriptions of these refined roles from external text files.\n",
        "\n",
        "### **6. Generate Refined Role Classification Prompt**\n",
        "- A second structured prompt is generated for **refined role classification**.\n",
        "- The model is tasked with answering **Yes/No** for each refined role.\n",
        "\n",
        "### **7. Predict Refined Roles Using LLM**\n",
        "- The script uses `get_refined_roles_predictions(prompt, refined_roles)` to determine which refined roles apply.\n",
        "- If the predicted refined roles exactly match the expected roles, the **exactmatch** counter is incremented.\n",
        "\n",
        "---\n",
        "\n",
        "## Functions & data structures used\n",
        "- **`main_roles_mapping`** *(dict)*: Defines the available refined roles for each main role category.\n",
        "- **`train_file_path`** *(str)*: Path to the training dataset.\n",
        "- **`val_file_path`** *(str)*: Path to the validation (development) dataset.\n",
        "- **`train_data_with_vectors`** *(list of dicts)*: Training dataset containing entity roles and semantic vectors.\n",
        "- **`data_with_vectors`** *(list of dicts)*: Validation/Test dataset used for evaluation containing entity roles and semantic vectors..\n",
        "- **`compute_cosine_similiraties`** *(function)*: Computes the similarity between an entity's semantic vector and training data.\n",
        "- **`get_top_k_per_main_role`** *(function)*: Retrieves the top `k=2` most similar training examples for each main role.\n",
        "- **`create_prompt`** *(function)*: Generates a structured classification prompt.\n",
        "- **`get_main_role_prediction`** *(function)*: Predicts the **main role** based on the prompt.\n",
        "- **`get_refined_roles_descriptions`** *(function)*: Loads textual descriptions of refined roles.\n",
        "- **`get_refined_roles_examples`** *(function)*: Retrieves example cases for refined role classification.\n",
        "- **`get_refined_roles_predictions`** *(function)*: Predicts the **refined roles** for the given entity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "3KQEiViv9LEc",
        "outputId": "4a89149f-7b2c-4d77-bc0a-cea702744c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted main role: Protagonist\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 5816 has 14.72 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 448.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-cd0629354fc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0mtype_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'refined'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_refined_roles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefined_roles_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_role\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m   \u001b[0mrefined_roles_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_refined_roles_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefined_roles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted refined roles: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefined_roles_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d7d59232d71b>\u001b[0m in \u001b[0;36mget_refined_roles_predictions\u001b[0;34m(prompt, refined_roles)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Run model inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Get logits for Yes/No\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    832\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 )\n\u001b[1;32m    588\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    590\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 5816 has 14.72 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 448.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "main_roles_mapping = {\n",
        "    \"protagonist\": [\"Guardian\", \"Martyr\", \"Peacemaker\", \"Rebel\", \"Underdog\", \"Virtuous\"],\n",
        "    \"antagonist\": [\n",
        "        \"Instigator\", \"Conspirator\", \"Tyrant\", \"Foreign Adversary\", \"Traitor\",\n",
        "        \"Spy\", \"Saboteur\", \"Corrupt\", \"Incompetent\", \"Terrorist\", \"Deceiver\", \"Bigot\"\n",
        "    ],\n",
        "    \"innocent\": [\"Forgotten\", \"Exploited\", \"Victim\", \"Scapegoat\"]\n",
        "}\n",
        "\n",
        "\n",
        "PHASE = 'TEST' #Possible values VALIDATION/TEST\n",
        "\n",
        "train_file_path = \"./Datasets/Train/EN/semantic_vectors_train.json\"\n",
        "train_data_with_vectors = read_file_into_list_of_dicts(train_file_path)\n",
        "if PHASE == 'VALIDATION':\n",
        "  correctly_predicted = 0\n",
        "  val_file_path = \"./Datasets/Development/EN/semantic_vectors_development.json\"\n",
        "  data_with_vectors = read_file_into_list_of_dicts(val_file_path)\n",
        "  #data_semantic_vectors = [item['semantic_vector'] for item in val_data_with_vectors]\n",
        "elif PHASE == 'TEST':\n",
        "  test_file_path = './Datasets/Test/EN/semantic_vectors_test.json'\n",
        "  data_with_vectors = read_file_into_list_of_dicts(test_file_path,train_val=False)\n",
        "  #data_semantic_vectors = [item['semantic_vector'] for item in test_data_with_vectors]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "i = 0\n",
        "exactmatch = 0.0\n",
        "\n",
        "\n",
        "for item in data_with_vectors:\n",
        "  train_data_with_vectors = compute_cosine_similiraties(item['semantic_vector'], train_data_with_vectors)\n",
        "\n",
        "  main_role_examples = get_top_k_per_main_role(train_data_with_vectors, k=2) # used for main_role inference\n",
        "\n",
        "  context_main_role = f\"\"\"<<SYS>>You are an expert in classification of narrative entities. Your task is to classify the entity framed between \"<T>\" and \"<\\T>\". You can choose just one label between innocent, protagonist, antagonist.\n",
        "  Assign the **Main Role** label based on the following criteria:\n",
        "\n",
        "  - protagonist: The central entity in the narrative, typically depicted as the main driver of events, actions, or decisions. This role is often associated with individuals, organizations, or groups that initiate key actions or are the primary focus of the story.\n",
        "\n",
        "  - antagonist: The entity that opposes, challenges, or creates obstacles for the protagonist or other actors in the narrative. The antagonist may act directly or indirectly, and can include individuals, organizations, groups, or abstract forces. This role is often linked to conflict or controversy within the story.\n",
        "\n",
        "  - innocent: An entity that is affected by the events of the narrative without playing an active role in driving them. The innocent may be a victim or a passive participant whose involvement is incidental rather than intentional. This role is typically associated with entities that experience consequences rather thancausethem.\n",
        "\n",
        "  In the Example section you have some examples<</SYS>>\"\"\"\n",
        "\n",
        "  type_role = 'main'\n",
        "  prompt = create_prompt(context_main_role, main_role_examples, item, type_role)\n",
        "\n",
        "  main_role_predicted = get_main_role_prediction(prompt)\n",
        "\n",
        "  if PHASE == 'VALIDATION':\n",
        "    if main_role_predicted.lower() == item['main_role'].strip():\n",
        "      correctly_predicted += 1.0\n",
        "\n",
        "    print(f\"\"\"Predicted main role: {main_role_predicted.strip()}\\nActual main role: {item['main_role'].strip().capitalize()}\"\"\")\n",
        "\n",
        "  elif PHASE == 'TEST':\n",
        "    print(f\"\"\"Predicted main role: {main_role_predicted.strip()}\"\"\")\n",
        "\n",
        "  refined_roles = main_roles_mapping[main_role_predicted.strip().lower()]\n",
        "\n",
        "  role=''\n",
        "  if main_role_predicted == 'Protagonist':\n",
        "    role='protagonist.txt'\n",
        "\n",
        "  elif main_role_predicted == 'Antagonist':\n",
        "    role='antagonist.txt'\n",
        "\n",
        "  elif main_role_predicted == 'Innocent':\n",
        "    role='innocent.txt'\n",
        "\n",
        "  descriptions_file_path = ...\n",
        "\n",
        "  refined_roles_descriptions = get_refined_roles_descriptions(main_role_predicted, descriptions_file_path)\n",
        "\n",
        "  context_refined_roles = f\"\"\"<<SYS>>You are an expert in classification of narrative entities. Your task is to understand which **Refined Roles** the entity framed between \"<T>\" and \"<\\T>\" plays in a narrative.\n",
        "  The **Refined Roles** are: {refined_roles}.\n",
        "  **Refine Roles** are described as follow: {refined_roles_descriptions}\\nFor each of the **Refined Roles** previously described, answer \"Yes\" if the entity framed plays the role in the **Narrative** or answer \"No\" if the entity does not play the role. Insert the prediction after \"**Refined Roles**:\". In the Example section you have some examples<</SYS>>\"\"\"\n",
        "\n",
        "  refined_roles_examples = get_refined_roles_examples(train_data_with_vectors, main_role_predicted.lower())\n",
        "\n",
        "\n",
        "  type_role = 'refined'\n",
        "  prompt = create_prompt(context_refined_roles, refined_roles_examples, item, type_role)\n",
        "  refined_roles_predicted = get_refined_roles_predictions(prompt, refined_roles)\n",
        "\n",
        "  print(\"Predicted refined roles: \", refined_roles_predicted)\n",
        "\n",
        "\n",
        "  if PHASE == 'VALIDATION':\n",
        "    print(\"Expected refined roles: \", item['refined_roles'], '\\n\\n')\n",
        "\n",
        "    if item['refined_roles'] == refined_roles_predicted:\n",
        "      exactmatch += 1\n",
        "  elif PHASE =='TEST':\n",
        "    item['refined_roles']=refined_roles_predicted\n",
        "    item['main_role']= main_role_predicted\n",
        "    refined = '\\t'.join(item['refined_roles'])\n",
        "    line = f\"{item['file_name']}\\t{item['entity_name']}\\t{item['start_offset']}\\t{item['end_offset']}\\t{item['main_role']}\\t{refined}\\n\"\n",
        "    print(line)\n",
        "\n",
        "\n",
        "if PHASE == 'VALIDATION':\n",
        "  print(f\"\"\"Corectly predicted:  {correctly_predicted}\"\"\")\n",
        "  print(f\"\"\"Total samples:  {len(data_with_vectors)}\"\"\")\n",
        "\n",
        "  print(f\"\"\"Main role predictions accuracy: {correctly_predicted/len(data_with_vectors)}\"\"\")\n",
        "  print(f\"\"\"Exact match: {exactmatch/len(data_with_vectors)}\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idKRuFQctR83"
      },
      "outputs": [],
      "source": [
        "def save_to_txt(data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for item in data:\n",
        "            # Format the line\n",
        "            refined = '\\t'.join(item['refined_roles'])\n",
        "            line = f\"{item['file_name']}\\t{item['entity_name']}\\t{item['start_offset']}\\t{item['end_offset']}\\t{item['main_role']}\\t{refined}\\n\"\n",
        "            file.write(line)\n",
        "\n",
        "if PHASE == 'TEST':\n",
        "    save_to_txt(data_with_vectors,'./results/results_task1EN_SemanticaInnovators.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3M47U6v4GyR6"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RobotLearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "051f1321d96a4f1187fdbb1ab21fc8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "183f6d92f98a495ca53e4228ba3f476f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d118b78f077c459986bcab0f72a1131e",
              "IPY_MODEL_8ff8d1b18f2e4c4d9be7ac1efd647653",
              "IPY_MODEL_553dba0372ac4e159352450d2c18851e"
            ],
            "layout": "IPY_MODEL_9c68e0525339432ab9df157bad026988"
          }
        },
        "187f5c18f080495a94023c8e13952496": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eef2cd81dec43db914a6a6df25b22eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fa2d0c633f34353b8f1adb036a3dbdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2285895a068942beb02d603b0153b0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "242daab895514656b2922a4dbb6a3a16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28eadc048d454e8cab5a92f882850d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b8a060125d748aa9ed490eb0ffe90f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6590290ef63e47f5a6571b16cc9f367c",
            "placeholder": "",
            "style": "IPY_MODEL_670b7a5122844f64b41d9752c514667b",
            "value": "1.38G/4.97G[00:32&lt;01:23,42.8MB/s]"
          }
        },
        "2c504dbe585b47c0bb9071c9b39bfddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03cddf9f42f4f7eb661b7c1d8f6514a",
            "placeholder": "",
            "style": "IPY_MODEL_051f1321d96a4f1187fdbb1ab21fc8fb",
            "value": "20.9k/20.9k[00:00&lt;00:00,1.40MB/s]"
          }
        },
        "2faa2c541b534c648eb0d83ef0539eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eef2cd81dec43db914a6a6df25b22eb",
            "max": 20919,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9214c1f98c7488eb676422ed71347e0",
            "value": 20919
          }
        },
        "3ec779308b27465dbff3d7184e4a93a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9619890a23b74241a430e6f43007e504",
              "IPY_MODEL_f93831b3b6be492f9349c5a63c8c52b8",
              "IPY_MODEL_2b8a060125d748aa9ed490eb0ffe90f0"
            ],
            "layout": "IPY_MODEL_eac20711608f4103a1b2ece366f7955e"
          }
        },
        "42f44926f5404158accb21489fe9d863": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf0fbc33dc8434f84ac5646d24d4340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f109858192d4658881f2b6afcad02ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ff0d07a2c3342c29b6c39ac0947e2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f37e0d5a6cc243629b56eca82589597b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eaea09a0359b4fc9b98478ae39bf2c80",
            "value": 0
          }
        },
        "553dba0372ac4e159352450d2c18851e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_573c3918a3bd408881f2b71a7951bebc",
            "placeholder": "",
            "style": "IPY_MODEL_4bf0fbc33dc8434f84ac5646d24d4340",
            "value": "844/844[00:00&lt;00:00,40.7kB/s]"
          }
        },
        "573c3918a3bd408881f2b71a7951bebc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bcad412518440068081e57089253a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6590290ef63e47f5a6571b16cc9f367c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670b7a5122844f64b41d9752c514667b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f5326ab73aa4e8686519361408ac48d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff8d1b18f2e4c4d9be7ac1efd647653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa2d0c633f34353b8f1adb036a3dbdf",
            "max": 844,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2285895a068942beb02d603b0153b0d7",
            "value": 844
          }
        },
        "9619890a23b74241a430e6f43007e504": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f5326ab73aa4e8686519361408ac48d",
            "placeholder": "",
            "style": "IPY_MODEL_9e94a829379a42f6877d032b65d921df",
            "value": "model-00001-of-00002.safetensors:28%"
          }
        },
        "9772a998fecf4fcdb38350c33f2d2ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c68e0525339432ab9df157bad026988": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e94a829379a42f6877d032b65d921df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fa48cd94c574738b6517565225829ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9214c1f98c7488eb676422ed71347e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bde07c073b7a4dc29a4caad15c4adbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f75e25a82cd841a4adb09af09f2fab30",
              "IPY_MODEL_2faa2c541b534c648eb0d83ef0539eed",
              "IPY_MODEL_2c504dbe585b47c0bb9071c9b39bfddd"
            ],
            "layout": "IPY_MODEL_4f109858192d4658881f2b6afcad02ba"
          }
        },
        "be7a10388759430fbf1017b6bfe91ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f6f476e71b4a22a50d351e059e5206",
            "placeholder": "",
            "style": "IPY_MODEL_28eadc048d454e8cab5a92f882850d97",
            "value": "Downloadingshards:0%"
          }
        },
        "cc1d0eee2a8d40c09973c68faa3f569b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d118b78f077c459986bcab0f72a1131e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa48cd94c574738b6517565225829ae",
            "placeholder": "",
            "style": "IPY_MODEL_ead4a2c2739240eeb12cd8cd2995d59a",
            "value": "config.json:100%"
          }
        },
        "d631df91a5234adaa8d9b919a8c3ab15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "decba6148a114addb7bb2d5237fe6c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9772a998fecf4fcdb38350c33f2d2ca1",
            "placeholder": "",
            "style": "IPY_MODEL_d631df91a5234adaa8d9b919a8c3ab15",
            "value": "0/2[00:32&lt;?,?it/s]"
          }
        },
        "e1262702b868466bb65fa81e20b16553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be7a10388759430fbf1017b6bfe91ea5",
              "IPY_MODEL_4ff0d07a2c3342c29b6c39ac0947e2ee",
              "IPY_MODEL_decba6148a114addb7bb2d5237fe6c68"
            ],
            "layout": "IPY_MODEL_187f5c18f080495a94023c8e13952496"
          }
        },
        "e7f6f476e71b4a22a50d351e059e5206": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac20711608f4103a1b2ece366f7955e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead4a2c2739240eeb12cd8cd2995d59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaea09a0359b4fc9b98478ae39bf2c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f03cddf9f42f4f7eb661b7c1d8f6514a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f37e0d5a6cc243629b56eca82589597b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f75e25a82cd841a4adb09af09f2fab30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_242daab895514656b2922a4dbb6a3a16",
            "placeholder": "",
            "style": "IPY_MODEL_5bcad412518440068081e57089253a45",
            "value": "model.safetensors.index.json:100%"
          }
        },
        "f93831b3b6be492f9349c5a63c8c52b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f44926f5404158accb21489fe9d863",
            "max": 4965799096,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc1d0eee2a8d40c09973c68faa3f569b",
            "value": 1384120320
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
